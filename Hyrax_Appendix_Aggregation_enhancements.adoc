= User-Specified Aggregation
:Leonard Porrello <lporrel@gmail.com>:
{docdate}
:numbered:
:toc:

== Introduction
 
In response to requests from NASA, and with their support, we have added
two new kinds of aggregation to Hyrax. Both of these aggregation
operations provide a way for client software to specify the granules
that will be used to build the aggregate result. While our existing
aggregation interface, based on NcML, works well for NASA's level 3 data
products, it is all but useless for level 2 __swath data__. These
aggregation functions are specifically designed to work with satellite
swath data without being limited to just swath data and are explicitly
intended for use with search interfaces that have knowledge of the
individual files that make up typical satellite data sets (often called
a __dataset inventory__).

NOTE: This service provides value-based subsetting for satellite
__swath data__. It's applicable to lots of other kinds of data, but
works best with data that meet certain requirements.

Providing search results that include explicit references to hundreds or
thousands of discrete files has been the only option for many search
interfaces up to this point. This is especially when the datasets holds
satellite swath data because swath data are not easily aggregated. For
this interface to Hyrax's aggregation software, we provide two kinds of
responses: Data in multiple files that are bundled together using an zip
archive and data in tabular form. For clients that request the aggregate
result in a zip file, given a request for values from N files, there
will be N entries in the resulting zip archive. Some of these entries
may simply indicate that no data matching the spatial or other
constraints were found. While the source data files can be in any format
that the Hyrax server can read, the response will be either netCDF3,
netCDF4 or ASCII. The netCDF3/4 files returned will conform to CF 1.6 to
the extent possible (the underlying data files may lack information CF
1.6 requires). For clients that request data in tabular form, the data
from N files will be returned in one ASCII CSV response. These values
can be easily assimilated by database systems, Excel and other tools.

== Intended Audience

This service was originally intended for software developers working
data search tools who need to be able to return results that encompass
hundreds or thousands of granules. It works best from a programmatic
interface, but it's certainly open to end users, see the examples using
curl for one way to access the service.

== Accessing the Aggregation Services

This 'service' is accessed using HTTP's GET or POST methods. In this
documentation I will describe how to use POST to send information, but
the same key-value parameters can be sent using the GET method, albeit
within the character limits of a URL (which vary depending on
implementation).

The service is accessed using the following set of key-value parameters:

operation::
  Use _operation_ to select from various kinds of responses. The form of
  the response also determines how the aggregation is built. The current
  values for this parameter are: _version_ which returns information
  about the service's version; _file_ returns a collection of files;
  __netcdf3__, __netcdf4__, _ascii_ all translate the underlying granule
  format to netcdf3, etc., and return that collection of translated
  files; _csv_ returns data from many granules as a single table of
  data, using Comma Separated Values (csv) format. More information
  about this is given below.
file::
  The URL path component to a granule served by Hyrax. This parameter
  will appear once for each file in the aggregation.
var::
  A comma-separated list of variables to include in the files returned
  when using _operation_ equal to __netcdf3__, __netcdf4__, __ascii__,
  or _csv_
bbox::
  Limit the values returned to those that fall within a bounding box for
  a given variable. Like __var__, this applies only to __netcdf3__,
  __netcdf4__, __ascii__, or _csv_

=== How to Use These Parameters

The _operation_ and _file_ parameters are the key to the service. By
listing multiple files, you can explicitly control which files are
accessed and the order of that access. The _operation_ parameter
provides a way to choose between a zipped response with many files
either in their native format (__file__) or in one of three well known
representations (__netcdf3__, netcdf4 _or_ ascii__).__

While a complete request can make use of only the _operation_ and _file_
parameters, adding the variable and value subsetting can provide a much
more manageable response. The _var_ and _bbox_ parameters can appear
either once or _N_ times where _N_ is the number of time the _file_
parameter appears. In the first case, the values of the single instances
of _var_ and/or _bbox_ are applied to every file/granule listed in the
request. In the second case the value of _var1_ is used with __file1__,
_var2_ with __file2__, and so on up to _varN_ and __fileN__. The same is
true of the _bbox_ parameter. Furthermore, these parameters act
independently, so a request can use one value for _var_ and _N_ values
for _bbox_ or vice versa.

=== Response Formats

This service will either return a collection of files bundled in a _zip
archive_ or it will return a since CSV/text file. When _operation_ is
__file__, __netcdf3__, _netcdf4_ or __ascii__, the service will take
each of the files as they are retrieve or built and put them in a zip
archive that it streams back to the client. The ZIP64(tm) format
extensions are used to overcome the size limitations of the original ZIP
format.

For the _csv_ operation, the response is a single CSV/text file.

=== More About _var_

The _var_ parameter is a comma-separated list of variables in the files
listed in the request. Each of the variables must be named just as it is
in the DAP dataset. If you're getting errors from the service that 'No
such variable exists in the dataset ...', use a web browser or _curl_ to
look at one of the granules and see what the exact name is. For many
NASA dataset, these names can be quite long and have several components,
separated by dots. One way to test the name is to build a URL to the
file and use the _getdap_ (part of the libdap software package) tool
like this

_getdap -d <url> -c_

If this returns an error, look at the DDS or DMR from the dataset and
figure out the correct name. Do that using

_getdap -d <url>_ or

_getdap4 -d <url>_

=== More About _bbox_

The _bbox_ parameter is probably the most powerful of the parameters in
terms of its ability to select specific data values. It has two
different modes, one when used with the zip-formatted responses (i.e.,
_operation_ is __netcdf3__, _netcdf4_ or __ascii__) and another when its
used with _operation_ equal to __csv__. However, ther are somethings
that are common to both uses of the parameter. In either case, _bbox_ is
used to select a range of values for a particular variable or a set of
variables. The format for a _bbox_ request has the following form

_[_ <lower value> _,_ <variable name> _,_ <upper value> _]_

for each variable in the subset request. If more than one variable is
included, use a series of _range requests_ surrounded by double quotes.
An example _box_ request looks like

&bbox="[49,Latitude,50][167,Longitude,170]"

which translates to __"for the variable Latitude, return only values
between 49 and 50 (inclusive) and for the variable Longitude return only
values between 167 and 170"__. Note that the example here uses two
variables named _Latitude_ and __Longitude__, but any variables in the
dataset could be used.

The _bbox_ operation is special, however, because the range limitation
applies not only to the variable listed, but to any other variables in
the request that share dimensions with those variables. Thus, for a
dataset that contains __Latitude__, _longitude_ and _Optical_Depth_
where all have the shared dimensions _x_ and __y__, the _bbox_ parameter
will choose values of _Latitude_ and _Longitude_ within the given values
and then apply the resulting bounding box to those variables and any
other variables that use the same named dimensions as those variables.
The named (i.e., __shared__) dimensions form the linkage between the
subsetting of the variables named in the _bbox_ value subset operation
and the other variables in the list of __var__s to return.

You can find out if variables in a dataset share named dimensions by
looking at the DDS (DAP2) or DMR (DAP4) for the dataset. Note that for
DAP4, in the example used in the previous paragraph, __Latitude__,
_longitude_ and _Optical_Depth_ form a 'coverage' where _Latitude_ and
_longitude_ are the domain and _Optical_Depth_ is the 'range'.

Note that the variables in the _bbox_ range requests must also be listed
in the _var_ parameter if you want their values to be returned.

The next two sections describe how the return format (zipped collection
of files or CSV table of data) affects the way the _bbox_ subset request
is interpreted.

==== _bbox & zip-formatted returns_

When the Aggregation Service is asked to provide a zipped collection of
files (__operation__ = __netcdf3__, _netcdf4_ or __ascii__), the
resulting data is stored as N-dimensional arrays in those kinds of
responses. This limits how _bbox_ can form subsets, particularly when
the values are in the form of 'swath data.' For this request type,
_bbox_ forms a bounding box for each variable in the list of range
requests and then forms the _union_ of those bounding boxes. For swath
data, this means that some extra values will be returned both because
the data rarely fit perfectly in a box for any given domain variable and
then the union of those two (imperfect) subsets usually results in some
data that are actually in neither bounding box. The _bbox_ operation
(which maps to a Hyrax server function) was designed to be liberal in
applying the subset to as to include all data points that meet the
subset criteria at the cost of including some that don't. The
alternative would be to exclude some matching data. Similarly, the
bounding box for the set of variables is the union for the same reason.
Hyrax contains server functions that can form both the union and
intersection of several bounding boxes returned by the _bbox_ function.

==== _bbox & the csv response_

The _csv_ response format is treated differently because the data values
are returned in a table and not arrays. Because of this, the
interpretation of _bbox_ is quite different. The subset request syntax
is interpreted as a set of _value filters_ that can be expressed as an
series of relational expressions that are combined using a logical AND
operation. Returning to the original example

&bbox="[49,Latitude,50][167,Longitude,170]"

a corresponding relational expression for this subset request would be

49 <= Latitude <= 50 AND 167 <= Longitude <= 170

Because the response is a single table, each variable named in the
request appears as a column. If there are _N_ variables listed in
__var__, then _N_ columns will appear in the resulting table (with one
potential exception where _N+1_ columns may appear). The filter
expression built from the _bbox_ subset request will be applied to each
row of this table, and only those rows with values that satisfy it will
be included in the output.

A tabular response like this implies that all of the values of a
particular row are related. For this kind of response (__operation__ =
__csv__) to work, each variable listed by use a common set of named
dimensions (i.e., shared dimensions). The one exception to this rule is
when the variables listed with _var_ fall into two groups, one of which
has _M_ dimensions (e.g., 2) and another group has _N_ (e.g., 3) and the
second group's named dimensions contains the first group's as a proper
subset. In this case, the extra dimension(s) of the second group will
appear as additional columns in the response. It sounds confusing, but
in practice it is pretty straightforward. Here's a concrete example.
Suppose a dataset has __Latitude__, _Longitude_ and
_Corrected_Optical_Depth_ and both Latitude and Longitude are two
dimensional arrays with named dimensions _x_ and _y_ and
Corrected_Optical_Depth is a three dimensional array with named
dimensions __Solution_3_Land__, _x_ and __y__. The _csv_ response would
include four columns, one each for Latitude, Longitude and
Corrected_Optical_Depth and a fourth for Solution_3_Land where the value
would be the index number.

== Performance and Implementation

Performance is linear in terms of the number of granules. The response
is streamed as it is built, so even very large responses use only a
little memory on the server. Of course, that won't be the case on the
client.

=== Implementation

The interface described here is built using a Servlet that talks to the
Hyrax BES - a C/C++ Unix daemon that reads and processes data building
the raw DAP2/4 response objects. The Servlet builds the response objects
it returns using the response objects returned by the BES. In the case
of the 'zipped files' response, the BES is told one by one to subset the
granules and return the result as __netcdf3__, et cetera. It streams
each returned file using a _ZipPOutputStream_ object from the Apache
Commons set of Java libraries. In the case of the _csv_ response the BES
is told to return the filtered data as ASCII and the servlet uses the
Java FilteredOutputStream class to strip away redundant header
information from the second, ..., Nth file/granule.

For each type of request, most of the work of subsetting the values is
performed by the BES, its constraint evaluator and a small set of server
functions. The server functions used for this service are:

roi::
  subsetting based on indices of shared dimensions
bbox::
  building bounding boxes described in array index space
bbox_union::
  building bounding boxes for forming the union or intersection of two
  or more bounding boxes
tabular::
  building a DAP Sequence from _N_ arrays, where when _N_ > 1, each
  array must be a member of the same DAP4 'coverage'

It is possible to access the essential functionality of the Aggregation
Service using these functions.

=== Design

////
This link (now an xref) does not appear to refer to an included document - ACP
link:./Use_cases_for_swath_and_time_series_aggregation.adoc[Aggregation Service]
////
The design of the <<use-cases-for-s-a-t-s-g, Aggregation Service>>
is documented as well, although some aspects of that document
are old and incorrect. it may also be useful to look at the source code,
which can be found on GitHub at https://github.com/opendap/olfs[olfs]
and https://github.com/opendap/bes[bes] in the
https://github.com/OPENDAP/olfs/tree/master/src/opendap/aggregation[aggregation]
and https://github.com/OPENDAP/bes/tree/master/functions[functions]
parts of those repos, respectively.

=== Examples

This section lists a number of examples of the aggregation service. We
have only a handful of data on our test server, but these examples
should work. Because the aggregation service is a machine interface, the
examples require that use of curl and text files that contain the POST
requests (except for the version operation).

=== Version

Request::
  http://test.opendap.org/dap/aggregation/?&operation=version

Returns::

----
Aggregation Interface Version: 1.1
<?xml version="1.0" encoding="UTF-8"?>
<response xmlns="http://xml.opendap.org/ns/bes/1.0#" reqID="[ajp-bio-…">
  <showVersion>
    <Administrator>support@opendap.org</Administrator>
    <library name="bes">3.16.0</library>
    <module name="dap-server/ascii">4.1.5</module>
    <module name="csv_handler">1.1.2</module>
    <library name="libdap">3.16.0</library>
…
</response>
----

==== Returning an Archive

*NB:* To get these examples, clone https://github.com/opendap/olfs, then
cd to __resources/aggregation/tests/demo__.

The example files are also available here:

* link:./extra_files/D1_netcdf3_variable_subset.txt[d1_netcdf3_variable_subset.txt]
* link:./extra_files/D2_netcdf3_bbox_subset.txt[d2_netcdf3_bbox_subset.txt]
* link:./extra_files/D3_csv_subset.txt[d3_csv_subset.txt]
* link:./extra_files/D4_csv_subset_dim.txt[d4_csv_subset_dim.txt]

In the OLFS repo on github, you'll see a file named
__resources/aggregation/tests/demo/short_names/d1_netcdf3_variable_subset.txt__.
Here's what it looks like:

----
edamame:demo jimg$ more short_names/d1_netcdf3_variable_subset.txt 
&operation=netcdf3
&var=Latitude,Longitude,Optical_Depth_Land_And_Ocean
&file=/data/modis/MOD04_L2.A2015021.0020.051.NRT.hdf
&file=/data/modis/MOD04_L2.A2015021.0025.051.NRT.hdf
&file=/data/modis/MOD04_L2.A2015021.0030.051.NRT.hdf
----

This example shows how the DAP2 projection constraint can be given once
and applied to a number of files. It's also possible to provide a unique
constraint for each file.

Each of the parameters begins with an ampersand (__&__). This command,
which will be sent to the service using POST, specifies the _netcdf3_
response, three files, and the DAP projection constraint
__Latitude,Longitude,Optical_Depth_Land_And_Ocean__. It may be that the
parameter name _&var_ is a bit misleading since you can actually provide
array subsetting there as well (but not the filtering-type DAP2/DAP4
constraints).

To send this command to the service, use _curl_ like this:

----
edamame:demo jimg$ curl -X POST -d @short_names/d1_netcdf3_variable_subset.txt http://test.opendap.org/opendap/aggregation > d1.zip
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current 
                                                     Dload  Upload   Total   Spent    Left  Speed
100  552k    0  552k  100   226   305k    124  0:00:01  0:00:01 --:--:--  305k
----

The output of _curl_ is redirected to a file (__d1.zip__) and we can
list its contents

----
verifying that it contains the files we expect.
----

----
edamame:demo jimg$ unzip -t d1.zip 
Archive:  d1.zip
    testing: MOD04_L2.A2015021.0020.051.NRT.hdf.nc   OK
    testing: MOD04_L2.A2015021.0025.051.NRT.hdf.nc   OK
    testing: MOD04_L2.A2015021.0030.051.NRT.hdf.nc   OK
No errors detected in compressed data of d1.zip.
----

==== Returning a Table

In this example, a request is made for data from the same three
variables from the same files, but the data are returned in a single
table. This request file is in the same directory as the previous
example.

The command file is close to the same as before, but uses the
_&operation_ or _csv_ and also adds a _&bbox_ command, the latter
provides a way to specify filtering based on latitude/longitude bounding
boxes.

----
edamame:demo jimg$ more short_names/d3_csv_subset.txt 
&operation=csv
&var=Latitude,Longitude,Image_Optical_Depth_Land_And_Ocean
&bbox="[49,Latitude,50][167,Longitude,170]"
&file=/data/modis/MOD04_L2.A2015021.0020.051.NRT.hdf
&file=/data/modis/MOD04_L2.A2015021.0025.051.NRT.hdf
&file=/data/modis/MOD04_L2.A2015021.0030.051.NRT.hdf
----

The command is sent using '__curl__ as before:

----
edamame:demo jimg$ curl -X POST -d @short_names/d3_csv_subset.txt http://test.opendap.org/opendap/aggregation > d3.csv
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  4141    0  3870  100   271   5150    360 --:--:-- --:--:-- --:--:--  5153
----

However, the response is now an ASCII table:

----
edamame:demo jimg$ more d3.csv
Dataset: function_result_MOD04_L2.A2015021.0020.051.NRT.hdf
table.Latitude, table.Longitude, table.Image_Optical_Depth_Land_And_Ocean
49.98, 169.598, -9999
49.9312, 169.82, -9999
49.9878, 169.119, -9999
49.9423, 169.331, -9999
49.8952, 169.548, -9999
49.8464, 169.77, -9999
49.7958, 169.998, -9999
49.9897, 168.659, -9999
49.9471, 168.862, -9999
...
----

== Potential Extensions to the Service

This service was purpose-built for the NASA CMR system, but it could be
extended in several useful ways.

* Support general DAP2 and DAP4 constraint expressions, including
function calls (functions are used behind the scenes already)
* Increased parallelism.
* Support for the *tar.gz* return type.
