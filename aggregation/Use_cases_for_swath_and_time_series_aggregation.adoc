[[use-cases-for-s-a-t-s-g]]
= Use cases for swath and time series aggregation
{docdate}
:numbered:
:toc:

Use cases for satellite Swath and Time Series aggregation. Our general 
approach is to use the Sequence data type to aggregate granules from 
Swath and Time series data sets (with themselves, not to mix the two, 
although the latter would be possible in general). Data will be read 
from arrays and loaded into a Sequence object, where it will be 
filtered and concatenated with other sequence objects. The result 
will be the aggregate. Of course, this will have to be optimized...

== Sample data
Level 3 are easy to find. Some examples:

* You can use Earthdata Search to find Level 3: +

** https://search.earthdata.nasa.gov/search?m=0.0703125!0.140625!2!1!0!&amp;ff=Subsetting+Services
** https://search.earthdata.nasa.gov/search?m=0.0703125!0.140625!2!1!0!&amp;ff=Subsetting+Services +
Click on the  icon next to any dataset and click on the "API Endpoints" 
tab. That will give you the OPeNDAP endpoint.

* For Level 2 data, here are a couple of examples:

** *GLAS:* The folders are all named GLA&lt;N&gt;* or GLAH&lt;N&gt;* 
N &gt;= 10 are L2 products (GLA10, GLA11, GLAH10, etc) 
http://nsidc.org/opendap/GLAS/contents.html
** *MODAPS (MODIS):*
*** FTP server is here: ftp://nrt1.modaps.eosdis.nasa.gov/allData/1/
*** Log in with your URS credentials
    (create an account here: https://urs.earthdata.nasa.gov)
*** Folders ending in _L2 contain level 2 data.

////
My notes:

* Level three data will be DAP2 Grids or DAP4 Coverages and look like they can easily be aggregated using NcML. We might think about a function that could aggregate them, but it's not in scope for this task.
* The GLAS data are stored in one-dimensional arrays. These are time series data: HDF5_GLOBAL.featureType: timeSeries. The GLAH files are HDF5 files. The one I looked at has 1Hz, 40Hz and 0.25Hz (4s) data. for each of the sample rates, there are a d_lat, d_lon and UTCTime arrays along with a large number of dependent variables in arrays. There are also some browse images. For some of the time series data there are two dims where the second dim provides cloud layer info (that is, values were gathered for cloud top and bottom for each of 10 layers.
** Suppose we want to aggregate a bunch of granules of these data? We can build a table of lat, lon, time[, cloud layer] and zero or more dependent variables for each granule, concatenate them and filter them. Optimizations include filtering before concatenating and (further) reading only data that would pass the filter in the first place.
** By including a granule name, and using nested Sequences, we can include useful metadata and make it easier to transform the resulting sequence back into an array. The nested Seq could be flattened for a return (as DAP binary or CSV).
* The MODIS data are typical MODIS L2 products with a number of dependent vars in 2D arrays and two 2D arrays, one for lat and lon. 
** We could read these data into a table with lat, lon and zero or more dependent values. Concatenate and filter. Optimizations are to read just the data needs and/or filter before concatenation. Could add granule and array index information to simplify transformation back from the seq to an array

I'm going to close this spike. The larger task in this sprint for this aggregation topic is to design the function; I'm going to write up some use cases and ask Patrick if they describe his needs.

Here are some URLs I used to get data:

* L2:
** ftp://nrt1.modaps.eosdis.nasa.gov/allData/1/MOD04_L2/2015/012/
** http://nsidc.org/opendap/GLAS/GLAH11.033/2003.02.21/contents.html
* L3:
* http://acdisc.gsfc.nasa.gov/opendap/Aqua_AIRS_Level3/AIRX3C28.005/2003/AIRS.2003.01.15.L3.CO2Std008.v5.4.12.70.X09264151518.hdf.html

////

== Use cases

* link:../index.php/Satellite_Swath_Data_Aggregation[Satellite
Swath Data Aggregation returned using CSV format]
* link:../index.php/Getting_a_tar_ball_instead_of_a_CSV_response[Satellite
Swath Data Aggregation returned using netCDF files]
* link:../index.php/Satellite_Time_Series_Aggregation[Satellite 
Time Series Aggregation]: This use case differs only 
slightly from the Satellite Swath 1 use case and I think it's 
pretty clear how to extrapolate the 'return as netCDF' version 
without writing it.

== Design

////
NOTE: this text is somewhat old and talk about only the CSV response. However, the same sequence diagram applies to the netCDF3/4 and 'file' responses.
////

There are two parts to both the CSV- and tar-ball-response solutions. 
As it happens, the two primary use cases - get swath data as CSV and 
get swath data in netCDF files - will likely be implemented using some 
different code in both the OLFS and BES. However, the narrative for 
both use cases' designs is roughly the same. A new web service endpoint 
will be made to process the requests. The requests will be made using 
HTTP POST when the list of granules will be enumerated in the request 
body and query parameters will supply the remaining information. 
Once the OLFS has parsed the information, it will use the BES to 
access the data and build the response, with two variations. 

For the CSV response, the BES will actually access each granule 
and combine the data from them into a single response. In this 
case the OLFS will call the BES once, passing in each granule 
using a BES request with multiple containers, one per granule, 
and requesting the response as CSV or ASCII. In addition, the 
BES will need to pass in the array variables as parameters of 
a server function that can form them into a table and a constraint 
expression that will select the requested space-time values.

For the netCDF response, the OLFS will iterate over the N granules, 
making N discrete requests for data from the named arrays within a 
space-time ROI. For each request, the OLFS will specify the return 
format as 'netCDF'. It will then collect the resulting netCDF 
response documents and bundle them using tar/gz or zip and return 
the result to the client.

////
Alternate version for the netCDF return implementation: 
We may be able to use the BES _stored result_ feature to eliminate 
the multiple trips to the BES. More investigation is needed.
////

link:"../images/a/a6/Aggregation_component.png"[Click here 
to see a component diagram for the aggregation service.]

=== BES support for the service

Here is a Sequence diagram for the BES code used to build the responses:

link:"../index.php/File:Aggregation_Sequnece.png"[Click here to see a 
sequence diagram for the aggregation server showing how the BES is
used by the service.] +
*Note:* The OLFS is not part of this interaction.

In the figure, the BES iterates over each (of _N_) container and calls
a function that takes _M_ arrays and forms them into a single Sequence 
(DAP2 or DAP4; both are implemented now). One important feature that 
is not shown in the diagram is that all of the data from the arrays 
are read into the sequence object, to be filtered later. It's possible 
that a later optimization might drop that, and some code used to build 
the netCDF response might help with that - see below. The initial 
version of the function will read all of the data from the arrays 
passed to it, assuming some later step will be used to filter them.

Once the data values are all read, each of the sequences is wrapped 
in a Structure (this is how the BES represents containers). The 
AggregationServer will then take all of the containers, get their 
sequences, and merge them into a single sequence.

The single sequence will be the sole top-level variable in the
response DDS/DMR. The ResponseBuilder object will serialize it,
routing the data through a CSV/ASCII transmitter.

==== Issues: DAP2

* Sequence::serialize will need to be specialized
** _hard_
** This is required to handle the case where other code loads all of the 
data values (the current version assumes that each row is read one at 
a time and the selection criteria applied then. Rows that don't match 
the selection criteria are never part of the Sequence object). The 
function, however, assumes that the sequence will filter out the 
unwanted values _after_ they have been loaded into memory. This could 
be quite complex.
* The AggregationServer must be written
** _easy_
** That the DDS with _N_ Structures, extract the Sequences and concatenate them.
* I'm not sure where in the DAP2 the CE Selection will be performed
** _spike_
** This could be important

==== Issues: DAP4

* D4Sequence::serialize must be specialized
** _easy to moderate_
** Along the lines of the DAP2 case, with the only real difference 
that the DAP4 sequence code is much simpler.
* CE Filters have yet to be implemented in DAP4
** _moderate_
** Must implement grammar, evaluation.
* I'm not sure where in the code the filter operation will be performed
** _spike_
** However, in DAP4, a server function is defined as building a new DMR 
to which the CE is then applied. Not sure how containers will affect this.
* The aggregationServer must be written
** _easy_
** ...and likely the same code as for DAP2
* Containers are not yet supported by the DMR class
** _moderate_
** Could copy the DDS implementation...

=== BES support for the netCDF tab-ball response
We will need to write a Server function that will take the Arrays and 
determine how to subset them to eliminate data not in the users ROI. 
Unless this not required - check on that. However, beyond this, no 
other code is needed in the BES for this response. The iteration over 
the granules will be handled by the OLFS.

=== OLFS support for the new web services

The OLFS will need to parse the inbound HTTP POST request, with the 
CSV and netCDF responses presenting distinct cases.

==== CSV case

For this case, the OLFS will build a single BES request where the 
various Query String parameters are moved into obvious places in the 
BES _setContainer_, _define_ and _get_ elements in the request 
command. Two differences between this command and 'typical' DAP 
command are that each of granules listed in the body of the HTTP 
Request Document will be a container in this command, and the command 
will specify an AggregationServer instance that will perform the 
aggregation operation. However, aside from the different HTTP request 
type (nb: we do support POST for DAP2 requests, albeit unofficially) 
and the different command, the basic back and forth of the OLFS and 
BES is not significantly different from a normal DAP service.

==== netCDF case

////
NOTE: We may use the BES store result feature to build this, and that 
would change the following.
////

For this request, the OLFS will receive a request that is structurally 
similar to the CSV web service HTTP Request Document - a POST with 
granule listed in the request document body and various other parameters
in the Query String. However, instead of building a single BES command,
the OLFS will build and issue _N_ commands, one for each granule. Each 
of these commands will request that the data be subset, maybe including 
a lat/lon/time constraint implemented as a server function, and returned
as (_returnAs_) a netCDF file. As the responses come back, it will save
off the resulting netcdf files so that they can be bundled up into a 
tar/gz or zip file and sent back to the user.

////
<!DOCTYPE html>
<html lang="en" dir="ltr" class="client-nojs">
<head>
<title>Use cases for swath and time series aggregation - OPeNDAP Documentation</title>
</head>
<body class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Use_cases_for_swath_and_time_series_aggregation skin-monobook action-view">
<div id="globalWrapper">
<div id="column-content"><div id="content" class="mw-body-primary" role="main">
	
	
	<h1 id="firstHeading" class="firstHeading" lang="en"><span dir="auto">Use cases for swath and time series aggregation</span></h1>
	<div id="bodyContent" class="mw-body">
		<div id="siteSub">From OPeNDAP Documentation</div>
		<div id="contentSub"></div>
		

		<!-- start content -->
<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><p><a href="../index.php/Development#General_Development" title="Development"> &lt;-- Back</a>
</p><p>Use cases for satellite Swath and Time Series aggregation. Our general approach is to use the Sequence data type to aggregate granules from Swath and Time series data sets (with themselves, not to mix the two, although the latter would be possible in general). Data will be read from arrays and loaded into a Sequence object, where it will be filtered and concatenated with other sequence objects. The result will be the aggregate. Of course, this will have to be optimized...
</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Sample_data"><span class="tocnumber">1</span> <span class="toctext">Sample data</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Use_cases"><span class="tocnumber">2</span> <span class="toctext">Use cases</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Design"><span class="tocnumber">3</span> <span class="toctext">Design</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="#BES_support_for_the_service"><span class="tocnumber">3.1</span> <span class="toctext">BES support for the service</span></a>
<ul>
<li class="toclevel-3 tocsection-5"><a href="#Issues:_DAP2"><span class="tocnumber">3.1.1</span> <span class="toctext">Issues: DAP2</span></a></li>
<li class="toclevel-3 tocsection-6"><a href="#Issues:_DAP4"><span class="tocnumber">3.1.2</span> <span class="toctext">Issues: DAP4</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-7"><a href="#BES_support_for_the_netCDF_tab-ball_response"><span class="tocnumber">3.2</span> <span class="toctext">BES support for the netCDF tab-ball response</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#OLFS_support_for_the_new_web_services"><span class="tocnumber">3.3</span> <span class="toctext">OLFS support for the new web services</span></a>
<ul>
<li class="toclevel-3 tocsection-9"><a href="#CSV_case"><span class="tocnumber">3.3.1</span> <span class="toctext">CSV case</span></a></li>
<li class="toclevel-3 tocsection-10"><a href="#netCDF_case"><span class="tocnumber">3.3.2</span> <span class="toctext">netCDF case</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

<h2><span class="mw-headline" id="Sample_data"><span class="mw-headline-number">1</span> Sample data</span></h2>
<ul>
<li> Level 3 are easy to find. Some examples:
</li>
</ul>
<p>You can use Earthdata Search to find Level 3:
<a rel="nofollow" class="external free" href="https://search.earthdata.nasa.gov/search?m=0.0703125!0.140625!2!1!0!&amp;ff=Subsetting+Services">https://search.earthdata.nasa.gov/search?m=0.0703125!0.140625!2!1!0!&amp;ff=Subsetting+Services</a>
Click on the  icon next to any dataset and click on the "API Endpoints" tab. That will give you the OPeNDAP endpoint.
</p>
<ul>
<li> For Level 2 data, here are a couple of examples:
<ul>
<li> GLAS: The folders are all named GLA&lt;N&gt;* or GLAH&lt;N&gt;* N &gt;= 10 are L2 products (GLA10, GLA11, GLAH10, etc) <a rel="nofollow" class="external free" href="http://nsidc.org/opendap/GLAS/contents.html">http://nsidc.org/opendap/GLAS/contents.html</a>
</li>
<li> MODAPS (MODIS): FTP server is here: <a rel="nofollow" class="external free" href="ftp://nrt1.modaps.eosdis.nasa.gov/allData/1/">ftp://nrt1.modaps.eosdis.nasa.gov/allData/1/</a>. Log in with your URS credentials (create an account here: <a rel="nofollow" class="external free" href="https://urs.earthdata.nasa.gov">https://urs.earthdata.nasa.gov</a>); Folders ending in _L2 contain level 2 data.
</li>
</ul>
</li>
</ul>
<p>My notes:
</p>
<ul>
<li> Level three data will be DAP2 Grids or DAP4 Coverages and look like they can easily be aggregated using NcML. We might think about a function that could aggregate them, but it's not in scope for this task.
</li>
<li> The GLAS data are stored in one-dimensional arrays. These are time series data: HDF5_GLOBAL.featureType: timeSeries. The GLAH files are HDF5 files. The one I looked at has 1Hz, 40Hz and 0.25Hz (4s) data. for each of the sample rates, there are a d_lat, d_lon and UTCTime arrays along with a large number of dependent variables in arrays. There are also some browse images. For some of the time series data there are two dims where the second dim provides cloud layer info (that is, values were gathered for cloud top and bottom for each of 10 layers.
<ul>
<li> Suppose we want to aggregate a bunch of granules of these data? We can build a table of lat, lon, time[, cloud layer] and zero or more dependent variables for each granule, concatenate them and filter them. Optimizations include filtering before concatenating and (further) reading only data that would pass the filter in the first place.
</li>
<li> By including a granule name, and using nested Sequences, we can include useful metadata and make it easier to transform the resulting sequence back into an array. The nested Seq could be flattened for a return (as DAP binary or CSV).
</li>
</ul>
</li>
<li> The MODIS data are typical MODIS L2 products with a number of dependent vars in 2D arrays and two 2D arrays, one for lat and lon. 
<ul>
<li> We could read these data into a table with lat, lon and zero or more dependent values. Concatenate and filter. Optimizations are to read just the data needs and/or filter before concatenation. Could add granule and array index information to simplify transformation back from the seq to an array
</li>
</ul>
</li>
</ul>
<p>I'm going to close this spike. The larger task in this sprint for this aggregation topic is to design the function; I'm going to write up some use cases and ask Patrick if they describe his needs.
</p><p>Here are some URLs I used to get data:
</p>
<ul>
<li> L2:
<ul>
<li> <a rel="nofollow" class="external free" href="ftp://nrt1.modaps.eosdis.nasa.gov/allData/1/MOD04_L2/2015/012/">ftp://nrt1.modaps.eosdis.nasa.gov/allData/1/MOD04_L2/2015/012/</a>
</li>
<li> <a rel="nofollow" class="external free" href="http://nsidc.org/opendap/GLAS/GLAH11.033/2003.02.21/contents.html">http://nsidc.org/opendap/GLAS/GLAH11.033/2003.02.21/contents.html</a>
</li>
</ul>
</li>
<li> L3:
<ul>
<li> <a rel="nofollow" class="external free" href="http://acdisc.gsfc.nasa.gov/opendap/Aqua_AIRS_Level3/AIRX3C28.005/2003/AIRS.2003.01.15.L3.CO2Std008.v5.4.12.70.X09264151518.hdf.html">http://acdisc.gsfc.nasa.gov/opendap/Aqua_AIRS_Level3/AIRX3C28.005/2003/AIRS.2003.01.15.L3.CO2Std008.v5.4.12.70.X09264151518.hdf.html</a>
</li>
</ul>
</li>
</ul>
<h2><span class="mw-headline" id="Use_cases"><span class="mw-headline-number">2</span> Use cases</span></h2>
<ul>
<li> <a href="../index.php/Satellite_Swath_Data_Aggregation" title="Satellite Swath Data Aggregation"> Satellite Swath Data Aggregation returned using CSV format</a>
</li>
<li> <a href="../index.php/Getting_a_tar_ball_instead_of_a_CSV_response" title="Getting a tar ball instead of a CSV response"> Satellite Swath Data Aggregation returned using netCDF files</a>
</li>
<li> <a href="../index.php/Satellite_Time_Series_Aggregation" title="Satellite Time Series Aggregation">Satellite Time Series Aggregation</a> This use case differs only slightly from the Satellite Swath 1 use case and I think it's pretty clear how to extrapolate the 'return as netCDF' version without writing it.
</li>
</ul>
<h2><span class="mw-headline" id="Design"><span class="mw-headline-number">3</span> Design</span></h2>
<p><font color="red"><i>Note that this text is somewhat old and talk about only the CSV response. However, the same sequence diagram applies to the netCDF3/4 and 'file' responses.</i></font>
</p><p>There are two parts to both the CSV- and tar-ball-response solutions. As it happens, the two primary use cases - get swath data as CSV and get swath data in netCDF files - will likely be implemented using some different code in both the OLFS and BES. However, the narrative for both use cases' designs is roughly the same. A new web service endpoint will be made to process the requests. The requests will be made using HTTP POST when the list of granules will be enumerated in the request body and query parameters will supply the remaining information. Once the OLFS has parsed the information, it will use the BES to access the data and build the response, with two variations. 
</p><p>For the CSV response, the BES will actually access each granule and combine the data from them into a single response. In this case the OLFS will call the BES once, passing in each granule using a BES request with multiple containers, one per granule, and requesting the response as CSV or ASCII. In addition, the BES will need to pass in the array variables as parameters of a server function that can form them into a table and a constraint expression that will select the requested space-time values.
</p><p>For the netCDF response, the OLFS will iterate over the N granules, making N discrete requests for data from the named arrays within a space-time ROI. For each request, the OLFS will specify the return format as 'netCDF'. It will then collect the resulting netCDF response documents and bundle them using tar/gz or zip and return the result to the client.
</p><p><strike>Alternate version for the netCDF return implementation: We may be able to use the BES <i>stored result</i> feature to eliminate the multiple trips to the BES. More investigation is needed.</strike>
</p>
<dl>
<dd><a href="../index.php/File:Aggregation_component.png" class="image" title="Component diagram for the aggregation service."><img alt="Component diagram for the aggregation service." src="../images/a/a6/Aggregation_component.png" width="505" height="420" /></a>
</dd>
</dl>
<h3><span class="mw-headline" id="BES_support_for_the_service"><span class="mw-headline-number">3.1</span> BES support for the service</span></h3>
<p>Here is a Sequence diagram for the BES code used to build the responses:
</p>
<dl>
<dd><a href="../index.php/File:Aggregation_Sequnece.png" class="image" title="Sequence diagram for the aggregation server showing how the BES is used by the service. Note that the OLFS is not part of this interaction."><img alt="Sequence diagram for the aggregation server showing how the BES is used by the service. Note that the OLFS is not part of this interaction." src="../images/d/dc/Aggregation_Sequnece.png" width="556" height="654" /></a>
</dd>
</dl>
<p>In the figure, the BES iterates over each (of <i>N</i>) container and calls a function that takes <i>M</i> arrays and forms them into a single Sequence (DAP2 or DAP4; both are implemented now). One important feature that is not shown in the diagram is that all of the data from the arrays are read into the sequence object, to be filtered later. It's possible that a later optimization might drop that, and some code used to build the netCDF response might help with that - see below. The initial version of the function will read all of the data from the arrays passed to it, assuming some later step will be used to filter them.
</p><p>Once the data values are all read, each of the sequences is wrapped in a Structure (this is how the BES represents containers). The AggregationServer will then take all of the containers, get their sequences, and merge them into a single sequence.
</p><p>The single sequence will be the sole top-level variable in the response DDS/DMR. The ResponseBuilder object will serialize it, routing the data through a CSV/ASCII transmitter.
</p>
<h4><span class="mw-headline" id="Issues:_DAP2"><span class="mw-headline-number">3.1.1</span> Issues: DAP2</span></h4>
<dl>
<dt>Sequence::serialize will need to be specialized</dt>
<dd> <i>hard</i>
</dd>
<dd>This is required to handle the case where other code loads all of the data values (the current version assumes that each row is read one at a time and the selection criteria applied then. Rows that don't match the selection criteria are never part of the Sequence object). The function, however, assumes that the sequence will filter out the unwanted values <i>after</i> they have been loaded into memory. This could be quite complex.
</dd>
<dt>The AggregationServer must be written</dt>
<dd> <i>easy</i>
</dd>
<dd>That the DDS with <i>N</i> Structures, extract the Sequences and concatenate them.
</dd>
<dt>I'm not sure where in the DAP2 the CE Selection will be performed</dt>
<dd> <i>spike</i>
</dd>
<dd>This could be important
</dd>
</dl>
<h4><span class="mw-headline" id="Issues:_DAP4"><span class="mw-headline-number">3.1.2</span> Issues: DAP4</span></h4>
<dl>
<dt>D4Sequence::serialize must be specialized</dt>
<dd> <i>easy to moderate</i>
</dd>
<dd>Along the lines of the DAP2 case, with the only real difference that the DAP4 sequence code is much simpler.
</dd>
<dt>CE Filters have yet to be implemented in DAP4</dt>
<dd> <i>moderate</i>
</dd>
<dd>Must implement grammar, evaluation.
</dd>
<dt>I'm not sure where in the code the filter operation will be performed</dt>
<dd> <i>spike</i>
</dd>
<dd>However, in DAP4, a server function is defined as building a new DMR to which the CE is then applied. Not sure how containers will affect this.
</dd>
<dt>The aggregationServer must be written</dt>
<dd> <i>easy</i>
</dd>
<dd>...and likely the same code as for DAP2
</dd>
<dt>Containers are not yet supported by the DMR class</dt>
<dd> <i>moderate</i>
</dd>
<dd>Could copy the DDS implementation...
</dd>
</dl>
<h3><span class="mw-headline" id="BES_support_for_the_netCDF_tab-ball_response"><span class="mw-headline-number">3.2</span> BES support for the netCDF tab-ball response</span></h3>
<p>We will need to write a Server function that will take the Arrays and determine how to subset them to eliminate data not in the users ROI. Unless this not required - check on that. However, beyond this, no other code is needed in the BES for this response. The iteration over the granules will be handled by the OLFS.
</p>
<h3><span class="mw-headline" id="OLFS_support_for_the_new_web_services"><span class="mw-headline-number">3.3</span> OLFS support for the new web services</span></h3>
<p>The OLFS will need to parse the inbound HTTP POST request, with the CSV and netCDF responses presenting distinct cases.
</p>
<h4><span class="mw-headline" id="CSV_case"><span class="mw-headline-number">3.3.1</span> CSV case</span></h4>
<p>For this case, the OLFS will build a single BES request where the various Query String parameters are moved into obvious places in the BES <i>setContainer</i>, <i>define</i> and <i>get</i> elements in the request command. Two differences between this command and 'typical' DAP command are that each of granules listed in the body of the HTTP Request Document will be a container in this command, and the command will specify an AggregationServer instance that will perform the aggregation operation. However, aside from the different HTTP request type (nb: we do support POST for DAP2 requests, albeit unofficially) and the different command, the basic back and forth of the OLFS and BES is not significantly different from a normal DAP service.
</p>
<h4><span class="mw-headline" id="netCDF_case"><span class="mw-headline-number">3.3.2</span> netCDF case</span></h4>
<p>NB: We may use the BES store result feature to build this, and that would change the following.
</p><p>For this request, the OLFS will receive a request that is structurally similar to the CSV web service HTTP Request Document - a POST with granule listed in the request document body and various other parameters in the Query String. However, instead of building a single BES command, the OLFS will build and issue <i>N</i> commands, one for each granule. Each of these commands will request that the data be subset, maybe including a lat/lon/time constraint implemented as a server function, and returned as (<i>returnAs</i>) a netCDF file. As the responses come back, it will save off the resulting netcdf files so that they can be bundled up into a tar/gz or zip file and sent back to the user.
</p>



</div>
				<!-- end content -->
				<div class="visualClear"></div>
	</div>
</div></div>
</body></html>
////